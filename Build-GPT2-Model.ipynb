{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fefd540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-03 05:28:36,046] torch.distributed.run: [WARNING] \n",
      "[2024-08-03 05:28:36,046] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-08-03 05:28:36,046] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-08-03 05:28:36,046] torch.distributed.run: [WARNING] *****************************************\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "total desired batch size: 524288\n",
      "=> calculated gradient accumulation steps: 16\n",
      "found 30 shards for split train\n",
      "found 1 shards for split val\n",
      "[rank4]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "[rank5]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "[rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "[rank2]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "[rank6]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "[rank3]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "[rank7]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "[rank1]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "step     0 | loss: 10.979298 | lr 8.3916e-07 | norm: 0.6385 | dt: 43557.48ms | tok/sec: 12036.69\n",
      "step     1 | loss: 10.980536 | lr 1.6783e-06 | norm: 0.6310 | dt: 21926.34ms | tok/sec: 23911.34\n",
      "step     2 | loss: 10.980339 | lr 2.5175e-06 | norm: 0.6248 | dt: 21850.62ms | tok/sec: 23994.19\n",
      "step     3 | loss: 10.979055 | lr 3.3566e-06 | norm: 0.6054 | dt: 21810.54ms | tok/sec: 24038.29\n",
      "step     4 | loss: 10.980097 | lr 4.1958e-06 | norm: 0.5706 | dt: 21810.27ms | tok/sec: 24038.58\n",
      "step     5 | loss: 10.979540 | lr 5.0350e-06 | norm: 0.5196 | dt: 21791.32ms | tok/sec: 24059.48\n",
      "step     6 | loss: 10.981082 | lr 5.8741e-06 | norm: 0.4654 | dt: 21864.74ms | tok/sec: 23978.70\n",
      "step     7 | loss: 10.979580 | lr 6.7133e-06 | norm: 0.4113 | dt: 21774.59ms | tok/sec: 24077.98\n",
      "step     8 | loss: 10.979521 | lr 7.5524e-06 | norm: 0.3803 | dt: 21809.84ms | tok/sec: 24039.06\n",
      "step     9 | loss: 10.980469 | lr 8.3916e-06 | norm: 0.3417 | dt: 21793.10ms | tok/sec: 24057.52\n",
      "step    10 | loss: 10.979528 | lr 9.2308e-06 | norm: 0.3024 | dt: 21843.36ms | tok/sec: 24002.17\n",
      "step    11 | loss: 10.979124 | lr 1.0070e-05 | norm: 0.2828 | dt: 21770.35ms | tok/sec: 24082.66\n",
      "step    12 | loss: 10.979918 | lr 1.0909e-05 | norm: 0.2562 | dt: 21811.27ms | tok/sec: 24037.49\n",
      "step    13 | loss: 10.976874 | lr 1.1748e-05 | norm: 0.2360 | dt: 21822.24ms | tok/sec: 24025.40\n",
      "step    14 | loss: 10.979437 | lr 1.2587e-05 | norm: 0.2233 | dt: 21957.93ms | tok/sec: 23876.94\n",
      "step    15 | loss: 10.977667 | lr 1.3427e-05 | norm: 0.2096 | dt: 21860.17ms | tok/sec: 23983.71\n",
      "step    16 | loss: 10.978324 | lr 1.4266e-05 | norm: 0.2053 | dt: 21833.40ms | tok/sec: 24013.11\n",
      "step    17 | loss: 10.978047 | lr 1.5105e-05 | norm: 0.1996 | dt: 21792.47ms | tok/sec: 24058.22\n",
      "step    18 | loss: 10.978124 | lr 1.5944e-05 | norm: 0.1928 | dt: 21838.56ms | tok/sec: 24007.45\n",
      "step    19 | loss: 10.979979 | lr 1.6783e-05 | norm: 0.1893 | dt: 21812.28ms | tok/sec: 24036.36\n",
      "step    20 | loss: 10.978965 | lr 1.7622e-05 | norm: 0.1856 | dt: 21836.84ms | tok/sec: 24009.34\n",
      "step    21 | loss: 10.978899 | lr 1.8462e-05 | norm: 0.1783 | dt: 21871.46ms | tok/sec: 23971.33\n",
      "step    22 | loss: 10.978492 | lr 1.9301e-05 | norm: 0.2111 | dt: 21841.38ms | tok/sec: 24004.35\n",
      "step    23 | loss: 10.978105 | lr 2.0140e-05 | norm: 0.1763 | dt: 21849.40ms | tok/sec: 23995.53\n",
      "step    24 | loss: 10.977905 | lr 2.0979e-05 | norm: 0.1651 | dt: 21838.51ms | tok/sec: 24007.50\n",
      "step    25 | loss: 10.979242 | lr 2.1818e-05 | norm: 0.1605 | dt: 21833.14ms | tok/sec: 24013.41\n",
      "step    26 | loss: 10.980284 | lr 2.2657e-05 | norm: 0.1576 | dt: 21827.62ms | tok/sec: 24019.47\n",
      "step    27 | loss: 10.978722 | lr 2.3497e-05 | norm: 0.1529 | dt: 21863.92ms | tok/sec: 23979.60\n",
      "step    28 | loss: 10.978882 | lr 2.4336e-05 | norm: 0.1502 | dt: 22072.07ms | tok/sec: 23753.45\n",
      "step    29 | loss: 10.978404 | lr 2.5175e-05 | norm: 0.1486 | dt: 21828.68ms | tok/sec: 24018.31\n",
      "step    30 | loss: 10.978876 | lr 2.6014e-05 | norm: 0.1485 | dt: 21795.47ms | tok/sec: 24054.91\n",
      "step    31 | loss: 10.978790 | lr 2.6853e-05 | norm: 0.1454 | dt: 21826.86ms | tok/sec: 24020.32\n",
      "step    32 | loss: 10.978220 | lr 2.7692e-05 | norm: 0.1458 | dt: 21872.76ms | tok/sec: 23969.91\n",
      "step    33 | loss: 10.978828 | lr 2.8531e-05 | norm: 0.1446 | dt: 21792.01ms | tok/sec: 24058.73\n",
      "step    34 | loss: 10.977650 | lr 2.9371e-05 | norm: 0.1434 | dt: 21835.30ms | tok/sec: 24011.03\n",
      "step    35 | loss: 10.979344 | lr 3.0210e-05 | norm: 0.1445 | dt: 21795.77ms | tok/sec: 24054.57\n",
      "step    36 | loss: 10.977404 | lr 3.1049e-05 | norm: 0.1440 | dt: 21863.39ms | tok/sec: 23980.18\n",
      "step    37 | loss: 10.978339 | lr 3.1888e-05 | norm: 0.1474 | dt: 21768.98ms | tok/sec: 24084.18\n",
      "step    38 | loss: 10.978845 | lr 3.2727e-05 | norm: 0.1758 | dt: 21855.35ms | tok/sec: 23989.01\n",
      "step    39 | loss: 10.979972 | lr 3.3566e-05 | norm: 0.1861 | dt: 21879.87ms | tok/sec: 23962.12\n",
      "step    40 | loss: 10.978584 | lr 3.4406e-05 | norm: 0.1561 | dt: 21822.11ms | tok/sec: 24025.54\n",
      "step    41 | loss: 10.979887 | lr 3.5245e-05 | norm: 0.1473 | dt: 21794.01ms | tok/sec: 24056.51\n",
      "step    42 | loss: 10.978110 | lr 3.6084e-05 | norm: 0.1526 | dt: 21824.54ms | tok/sec: 24022.87\n",
      "step    43 | loss: 10.978155 | lr 3.6923e-05 | norm: 0.1550 | dt: 21815.22ms | tok/sec: 24033.12\n",
      "step    44 | loss: 10.979256 | lr 3.7762e-05 | norm: 0.1546 | dt: 21817.46ms | tok/sec: 24030.66\n",
      "step    45 | loss: 10.979489 | lr 3.8601e-05 | norm: 0.1572 | dt: 21523.61ms | tok/sec: 24358.74\n",
      "step    46 | loss: 10.978443 | lr 3.9441e-05 | norm: 0.1565 | dt: 21167.26ms | tok/sec: 24768.82\n",
      "step    47 | loss: 10.978113 | lr 4.0280e-05 | norm: 0.1576 | dt: 21185.45ms | tok/sec: 24747.56\n",
      "step    48 | loss: 10.980008 | lr 4.1119e-05 | norm: 0.1615 | dt: 21167.68ms | tok/sec: 24768.32\n",
      "step    49 | loss: 10.979643 | lr 4.1958e-05 | norm: 0.1642 | dt: 21143.79ms | tok/sec: 24796.31\n",
      "step    50 | loss: 10.979860 | lr 4.2797e-05 | norm: 0.1660 | dt: 21178.34ms | tok/sec: 24755.86\n",
      "step    51 | loss: 10.979278 | lr 4.3636e-05 | norm: 0.1686 | dt: 21137.64ms | tok/sec: 24803.52\n",
      "step    52 | loss: 10.978127 | lr 4.4476e-05 | norm: 0.1696 | dt: 21187.66ms | tok/sec: 24744.97\n",
      "step    53 | loss: 10.978298 | lr 4.5315e-05 | norm: 0.1699 | dt: 21119.66ms | tok/sec: 24824.64\n",
      "step    54 | loss: 10.979505 | lr 4.6154e-05 | norm: 0.1715 | dt: 21140.25ms | tok/sec: 24800.46\n",
      "step    55 | loss: 10.979239 | lr 4.6993e-05 | norm: 0.1727 | dt: 21655.83ms | tok/sec: 24210.01\n",
      "step    56 | loss: 10.978329 | lr 4.7832e-05 | norm: 0.1752 | dt: 21784.85ms | tok/sec: 24066.63\n",
      "step    57 | loss: 10.978664 | lr 4.8671e-05 | norm: 0.1790 | dt: 21762.84ms | tok/sec: 24090.97\n",
      "step    58 | loss: 10.978518 | lr 4.9510e-05 | norm: 0.1797 | dt: 21748.54ms | tok/sec: 24106.81\n",
      "step    59 | loss: 10.979957 | lr 5.0350e-05 | norm: 0.1819 | dt: 21768.18ms | tok/sec: 24085.07\n",
      "step    60 | loss: 10.977099 | lr 5.1189e-05 | norm: 0.1812 | dt: 21745.42ms | tok/sec: 24110.27\n",
      "step    61 | loss: 10.977642 | lr 5.2028e-05 | norm: 0.1822 | dt: 21766.36ms | tok/sec: 24087.08\n",
      "step    62 | loss: 10.978274 | lr 5.2867e-05 | norm: 0.1829 | dt: 21753.60ms | tok/sec: 24101.21\n",
      "step    63 | loss: 10.978672 | lr 5.3706e-05 | norm: 0.1900 | dt: 21747.18ms | tok/sec: 24108.32\n",
      "step    64 | loss: 10.979003 | lr 5.4545e-05 | norm: 0.1908 | dt: 21780.23ms | tok/sec: 24071.74\n",
      "step    65 | loss: 10.978275 | lr 5.5385e-05 | norm: 0.2064 | dt: 21799.67ms | tok/sec: 24050.28\n",
      "step    66 | loss: 10.979126 | lr 5.6224e-05 | norm: 0.2095 | dt: 21814.19ms | tok/sec: 24034.26\n",
      "step    67 | loss: 10.979921 | lr 5.7063e-05 | norm: 0.2158 | dt: 21813.82ms | tok/sec: 24034.67\n",
      "step    68 | loss: 10.980472 | lr 5.7902e-05 | norm: 0.2355 | dt: 21812.13ms | tok/sec: 24036.54\n",
      "step    69 | loss: 10.979716 | lr 5.8741e-05 | norm: 0.2355 | dt: 21788.03ms | tok/sec: 24063.12\n",
      "step    70 | loss: 10.980729 | lr 5.9580e-05 | norm: 0.2385 | dt: 21768.45ms | tok/sec: 24084.77\n",
      "step    71 | loss: 10.979954 | lr 6.0420e-05 | norm: 0.2432 | dt: 21772.09ms | tok/sec: 24080.74\n",
      "step    72 | loss: 10.979976 | lr 6.1259e-05 | norm: 0.2431 | dt: 21853.41ms | tok/sec: 23991.13\n",
      "step    73 | loss: 10.979795 | lr 6.2098e-05 | norm: 0.2395 | dt: 21819.23ms | tok/sec: 24028.71\n",
      "step    74 | loss: 10.980139 | lr 6.2937e-05 | norm: 0.2805 | dt: 21810.28ms | tok/sec: 24038.57\n",
      "step    75 | loss: 10.977200 | lr 6.3776e-05 | norm: 0.2797 | dt: 21761.44ms | tok/sec: 24092.52\n",
      "step    76 | loss: 10.979818 | lr 6.4615e-05 | norm: 0.2466 | dt: 21794.73ms | tok/sec: 24055.72\n",
      "step    77 | loss: 10.979430 | lr 6.5455e-05 | norm: 0.2299 | dt: 21778.48ms | tok/sec: 24073.67\n",
      "step    78 | loss: 10.980823 | lr 6.6294e-05 | norm: 0.2275 | dt: 21795.53ms | tok/sec: 24054.84\n",
      "step    79 | loss: 10.980287 | lr 6.7133e-05 | norm: 0.2197 | dt: 21775.02ms | tok/sec: 24077.50\n",
      "step    80 | loss: 10.980941 | lr 6.7972e-05 | norm: 0.3233 | dt: 21829.09ms | tok/sec: 24017.86\n",
      "step    81 | loss: 10.980234 | lr 6.8811e-05 | norm: 0.3142 | dt: 21824.07ms | tok/sec: 24023.38\n",
      "step    82 | loss: 10.981190 | lr 6.9650e-05 | norm: 0.2578 | dt: 21816.08ms | tok/sec: 24032.18\n",
      "step    83 | loss: 10.980967 | lr 7.0490e-05 | norm: 0.2493 | dt: 21818.14ms | tok/sec: 24029.91\n",
      "step    84 | loss: 10.980980 | lr 7.1329e-05 | norm: 0.2444 | dt: 21840.77ms | tok/sec: 24005.02\n",
      "step    85 | loss: 10.979621 | lr 7.2168e-05 | norm: 0.2761 | dt: 21799.81ms | tok/sec: 24050.12\n",
      "step    86 | loss: 10.979856 | lr 7.3007e-05 | norm: 0.2977 | dt: 21805.58ms | tok/sec: 24043.76\n",
      "step    87 | loss: 10.981225 | lr 7.3846e-05 | norm: 0.3009 | dt: 21803.56ms | tok/sec: 24045.98\n",
      "step    88 | loss: 10.980913 | lr 7.4685e-05 | norm: 0.3074 | dt: 21834.53ms | tok/sec: 24011.88\n",
      "step    89 | loss: 10.981556 | lr 7.5524e-05 | norm: 0.3288 | dt: 21860.65ms | tok/sec: 23983.18\n",
      "step    90 | loss: 10.980153 | lr 7.6364e-05 | norm: 0.3325 | dt: 21832.67ms | tok/sec: 24013.92\n",
      "step    91 | loss: 10.980391 | lr 7.7203e-05 | norm: 0.3424 | dt: 21820.61ms | tok/sec: 24027.20\n",
      "step    92 | loss: 10.981408 | lr 7.8042e-05 | norm: 0.3534 | dt: 21799.22ms | tok/sec: 24050.77\n",
      "step    93 | loss: 10.981836 | lr 7.8881e-05 | norm: 0.3526 | dt: 21783.40ms | tok/sec: 24068.24\n",
      "step    94 | loss: 10.982078 | lr 7.9720e-05 | norm: 0.3599 | dt: 21817.61ms | tok/sec: 24030.50\n",
      "step    95 | loss: 10.981891 | lr 8.0559e-05 | norm: 0.3514 | dt: 22152.16ms | tok/sec: 23667.58\n",
      "step    96 | loss: 10.980936 | lr 8.1399e-05 | norm: 0.3505 | dt: 21834.24ms | tok/sec: 24012.19\n",
      "step    97 | loss: 10.981443 | lr 8.2238e-05 | norm: 0.3458 | dt: 21800.98ms | tok/sec: 24048.83\n",
      "step    98 | loss: 10.980167 | lr 8.3077e-05 | norm: 0.3521 | dt: 21825.09ms | tok/sec: 24022.26\n",
      "step    99 | loss: 10.980113 | lr 8.3916e-05 | norm: 0.3397 | dt: 21783.74ms | tok/sec: 24067.86\n",
      "step   100 | loss: 10.980693 | lr 8.4755e-05 | norm: 0.3310 | dt: 21794.77ms | tok/sec: 24055.68\n",
      "step   101 | loss: 10.982523 | lr 8.5594e-05 | norm: 0.3115 | dt: 21748.18ms | tok/sec: 24107.21\n",
      "step   102 | loss: 10.982368 | lr 8.6434e-05 | norm: 0.3227 | dt: 21788.56ms | tok/sec: 24062.53\n",
      "step   103 | loss: 10.979777 | lr 8.7273e-05 | norm: 0.3619 | dt: 21760.68ms | tok/sec: 24093.37\n",
      "step   104 | loss: 10.982028 | lr 8.8112e-05 | norm: 0.3049 | dt: 21757.56ms | tok/sec: 24096.82\n",
      "step   105 | loss: 10.981696 | lr 8.8951e-05 | norm: 0.2995 | dt: 21769.34ms | tok/sec: 24083.78\n",
      "step   106 | loss: 10.982019 | lr 8.9790e-05 | norm: 0.3681 | dt: 21818.34ms | tok/sec: 24029.70\n",
      "step   107 | loss: 10.982607 | lr 9.0629e-05 | norm: 0.4134 | dt: 21721.26ms | tok/sec: 24137.09\n",
      "step   108 | loss: 10.981679 | lr 9.1469e-05 | norm: 0.3591 | dt: 21800.30ms | tok/sec: 24049.58\n",
      "step   109 | loss: 10.980388 | lr 9.2308e-05 | norm: 0.3376 | dt: 21693.53ms | tok/sec: 24167.95\n",
      "step   110 | loss: 10.981738 | lr 9.3147e-05 | norm: 0.3404 | dt: 21804.93ms | tok/sec: 24044.47\n",
      "step   111 | loss: 10.982742 | lr 9.3986e-05 | norm: 0.3599 | dt: 21696.37ms | tok/sec: 24164.77\n",
      "step   112 | loss: 10.981801 | lr 9.4825e-05 | norm: 0.3864 | dt: 21774.07ms | tok/sec: 24078.54\n",
      "step   113 | loss: 10.981653 | lr 9.5664e-05 | norm: 0.4158 | dt: 21740.56ms | tok/sec: 24115.67\n",
      "step   114 | loss: 10.983106 | lr 9.6503e-05 | norm: 0.4504 | dt: 21858.74ms | tok/sec: 23985.29\n",
      "step   115 | loss: 10.983422 | lr 9.7343e-05 | norm: 0.4199 | dt: 21831.31ms | tok/sec: 24015.41\n",
      "step   116 | loss: 10.981852 | lr 9.8182e-05 | norm: 0.4078 | dt: 21825.06ms | tok/sec: 24022.29\n",
      "step   117 | loss: 10.981715 | lr 9.9021e-05 | norm: 0.3964 | dt: 21779.07ms | tok/sec: 24073.02\n",
      "step   118 | loss: 10.981768 | lr 9.9860e-05 | norm: 0.3940 | dt: 21816.11ms | tok/sec: 24032.14\n",
      "step   119 | loss: 10.983792 | lr 1.0070e-04 | norm: 0.3850 | dt: 21770.83ms | tok/sec: 24082.13\n",
      "step   120 | loss: 10.981725 | lr 1.0154e-04 | norm: 0.3741 | dt: 21829.68ms | tok/sec: 24017.21\n",
      "step   121 | loss: 10.982960 | lr 1.0238e-04 | norm: 0.3471 | dt: 21798.72ms | tok/sec: 24051.32\n",
      "step   122 | loss: 10.983373 | lr 1.0322e-04 | norm: 0.3295 | dt: 21815.84ms | tok/sec: 24032.45\n",
      "step   123 | loss: 10.983081 | lr 1.0406e-04 | norm: 0.3609 | dt: 21765.81ms | tok/sec: 24087.68\n",
      "step   124 | loss: 10.986257 | lr 1.0490e-04 | norm: 0.3683 | dt: 21790.03ms | tok/sec: 24060.91\n",
      "step   125 | loss: 10.982700 | lr 1.0573e-04 | norm: 0.3534 | dt: 21803.88ms | tok/sec: 24045.63\n",
      "step   126 | loss: 10.985344 | lr 1.0657e-04 | norm: 0.3619 | dt: 21825.34ms | tok/sec: 24021.99\n",
      "step   127 | loss: 10.983394 | lr 1.0741e-04 | norm: 0.4048 | dt: 21789.09ms | tok/sec: 24061.95\n",
      "step   128 | loss: 10.983294 | lr 1.0825e-04 | norm: 0.4156 | dt: 21805.69ms | tok/sec: 24043.63\n",
      "step   129 | loss: 10.984357 | lr 1.0909e-04 | norm: 0.3871 | dt: 21767.88ms | tok/sec: 24085.39\n",
      "step   130 | loss: 10.983027 | lr 1.0993e-04 | norm: 0.3709 | dt: 21811.20ms | tok/sec: 24037.56\n",
      "step   131 | loss: 10.986476 | lr 1.1077e-04 | norm: 0.3894 | dt: 21761.43ms | tok/sec: 24092.54\n",
      "step   132 | loss: 10.985757 | lr 1.1161e-04 | norm: 0.4121 | dt: 21804.47ms | tok/sec: 24044.98\n",
      "step   133 | loss: 10.985283 | lr 1.1245e-04 | norm: 0.4481 | dt: 21767.61ms | tok/sec: 24085.69\n",
      "step   134 | loss: 10.983513 | lr 1.1329e-04 | norm: 0.4511 | dt: 21771.79ms | tok/sec: 24081.07\n",
      "step   135 | loss: 10.987044 | lr 1.1413e-04 | norm: 0.4314 | dt: 21563.98ms | tok/sec: 24313.14\n",
      "step   136 | loss: 10.985411 | lr 1.1497e-04 | norm: 0.4235 | dt: 21569.05ms | tok/sec: 24307.42\n",
      "step   137 | loss: 10.983190 | lr 1.1580e-04 | norm: 0.4195 | dt: 21548.56ms | tok/sec: 24330.54\n",
      "step   138 | loss: 10.987455 | lr 1.1664e-04 | norm: 0.4104 | dt: 21604.63ms | tok/sec: 24267.39\n",
      "step   139 | loss: 10.985860 | lr 1.1748e-04 | norm: 0.4067 | dt: 21550.59ms | tok/sec: 24328.25\n",
      "step   140 | loss: 10.983830 | lr 1.1832e-04 | norm: 0.3895 | dt: 21577.76ms | tok/sec: 24297.61\n",
      "step   141 | loss: 10.985487 | lr 1.1916e-04 | norm: 0.3847 | dt: 21574.49ms | tok/sec: 24301.30\n",
      "step   142 | loss: 10.985765 | lr 1.2000e-04 | norm: 0.4019 | dt: 21641.82ms | tok/sec: 24225.69\n",
      "step   143 | loss: 10.984346 | lr 1.2084e-04 | norm: 0.4199 | dt: 21563.24ms | tok/sec: 24313.97\n",
      "step   144 | loss: 10.986224 | lr 1.2168e-04 | norm: 0.4088 | dt: 21645.48ms | tok/sec: 24221.59\n",
      "step   145 | loss: 10.987579 | lr 1.2252e-04 | norm: 0.4441 | dt: 21579.59ms | tok/sec: 24295.55\n",
      "step   146 | loss: 10.987516 | lr 1.2336e-04 | norm: 0.4655 | dt: 21553.56ms | tok/sec: 24324.90\n",
      "step   147 | loss: 10.987585 | lr 1.2420e-04 | norm: 0.4441 | dt: 21555.11ms | tok/sec: 24323.14\n",
      "step   148 | loss: 10.984509 | lr 1.2503e-04 | norm: 0.4352 | dt: 21609.72ms | tok/sec: 24261.67\n",
      "step   149 | loss: 10.986368 | lr 1.2587e-04 | norm: 0.4428 | dt: 21529.49ms | tok/sec: 24352.09\n",
      "step   150 | loss: 10.986752 | lr 1.2671e-04 | norm: 0.4727 | dt: 21595.07ms | tok/sec: 24278.13\n",
      "step   151 | loss: 10.986448 | lr 1.2755e-04 | norm: 0.4983 | dt: 21523.77ms | tok/sec: 24358.56\n",
      "step   152 | loss: 10.986270 | lr 1.2839e-04 | norm: 0.4952 | dt: 21560.96ms | tok/sec: 24316.54\n",
      "step   153 | loss: 10.986266 | lr 1.2923e-04 | norm: 0.4764 | dt: 21562.32ms | tok/sec: 24315.01\n",
      "step   154 | loss: 10.986558 | lr 1.3007e-04 | norm: 0.4712 | dt: 21643.91ms | tok/sec: 24223.35\n",
      "step   155 | loss: 10.985853 | lr 1.3091e-04 | norm: 0.4587 | dt: 21598.81ms | tok/sec: 24273.94\n",
      "step   156 | loss: 10.987424 | lr 1.3175e-04 | norm: 0.4498 | dt: 21622.93ms | tok/sec: 24246.86\n",
      "step   157 | loss: 10.988764 | lr 1.3259e-04 | norm: 0.4437 | dt: 21582.12ms | tok/sec: 24292.70\n",
      "step   158 | loss: 10.989452 | lr 1.3343e-04 | norm: 0.4606 | dt: 21594.82ms | tok/sec: 24278.41\n",
      "step   159 | loss: 10.986687 | lr 1.3427e-04 | norm: 0.4721 | dt: 21550.71ms | tok/sec: 24328.11\n",
      "step   160 | loss: 10.986034 | lr 1.3510e-04 | norm: 0.4633 | dt: 21585.02ms | tok/sec: 24289.44\n",
      "step   161 | loss: 10.985076 | lr 1.3594e-04 | norm: 0.5030 | dt: 21530.07ms | tok/sec: 24351.43\n",
      "step   162 | loss: 10.987595 | lr 1.3678e-04 | norm: 0.5223 | dt: 21624.00ms | tok/sec: 24245.65\n",
      "step   163 | loss: 10.985036 | lr 1.3762e-04 | norm: 0.4943 | dt: 21586.91ms | tok/sec: 24287.31\n",
      "step   164 | loss: 10.987377 | lr 1.3846e-04 | norm: 0.4872 | dt: 21590.42ms | tok/sec: 24283.37\n",
      "step   165 | loss: 10.990246 | lr 1.3930e-04 | norm: 0.5138 | dt: 21563.58ms | tok/sec: 24313.59\n",
      "step   166 | loss: 10.984720 | lr 1.4014e-04 | norm: 0.5413 | dt: 21574.24ms | tok/sec: 24301.58\n",
      "step   167 | loss: 10.987671 | lr 1.4098e-04 | norm: 0.5470 | dt: 21566.59ms | tok/sec: 24310.19\n",
      "step   168 | loss: 10.987517 | lr 1.4182e-04 | norm: 0.5249 | dt: 21602.93ms | tok/sec: 24269.30\n",
      "step   169 | loss: 10.986758 | lr 1.4266e-04 | norm: 0.5177 | dt: 21550.07ms | tok/sec: 24328.83\n",
      "step   170 | loss: 10.984919 | lr 1.4350e-04 | norm: 0.5109 | dt: 21596.98ms | tok/sec: 24275.99\n",
      "step   171 | loss: 10.990987 | lr 1.4434e-04 | norm: 0.5048 | dt: 21589.89ms | tok/sec: 24283.96\n",
      "step   172 | loss: 10.987741 | lr 1.4517e-04 | norm: 0.5084 | dt: 21612.77ms | tok/sec: 24258.25\n",
      "step   173 | loss: 10.986959 | lr 1.4601e-04 | norm: 0.5204 | dt: 21558.48ms | tok/sec: 24319.34\n",
      "step   174 | loss: 10.991534 | lr 1.4685e-04 | norm: 0.5304 | dt: 21610.65ms | tok/sec: 24260.63\n",
      "step   175 | loss: 10.994340 | lr 1.4769e-04 | norm: 0.5435 | dt: 21539.10ms | tok/sec: 24341.22\n",
      "step   176 | loss: 10.989010 | lr 1.4853e-04 | norm: 0.5668 | dt: 21656.71ms | tok/sec: 24209.03\n",
      "step   177 | loss: 10.989279 | lr 1.4937e-04 | norm: 0.5624 | dt: 21588.67ms | tok/sec: 24285.33\n",
      "step   178 | loss: 10.990455 | lr 1.5021e-04 | norm: 0.5536 | dt: 21642.00ms | tok/sec: 24225.49\n",
      "step   179 | loss: 10.991537 | lr 1.5105e-04 | norm: 0.5724 | dt: 21620.53ms | tok/sec: 24249.55\n",
      "step   180 | loss: 10.992225 | lr 1.5189e-04 | norm: 0.5871 | dt: 21676.83ms | tok/sec: 24186.56\n",
      "step   181 | loss: 10.990547 | lr 1.5273e-04 | norm: 0.5888 | dt: 21598.71ms | tok/sec: 24274.05\n",
      "step   182 | loss: 10.989401 | lr 1.5357e-04 | norm: 0.5894 | dt: 21641.31ms | tok/sec: 24226.26\n",
      "step   183 | loss: 10.993550 | lr 1.5441e-04 | norm: 0.5840 | dt: 21596.20ms | tok/sec: 24276.87\n",
      "step   184 | loss: 10.989466 | lr 1.5524e-04 | norm: 0.5777 | dt: 21653.35ms | tok/sec: 24212.79\n",
      "step   185 | loss: 10.994835 | lr 1.5608e-04 | norm: 0.5758 | dt: 21615.55ms | tok/sec: 24255.13\n",
      "step   186 | loss: 10.989422 | lr 1.5692e-04 | norm: 0.5821 | dt: 21641.89ms | tok/sec: 24225.61\n",
      "step   187 | loss: 10.991399 | lr 1.5776e-04 | norm: 0.5936 | dt: 21603.20ms | tok/sec: 24268.99\n",
      "step   188 | loss: 10.993424 | lr 1.5860e-04 | norm: 0.6007 | dt: 21662.88ms | tok/sec: 24202.14\n",
      "step   189 | loss: 10.994353 | lr 1.5944e-04 | norm: 0.6245 | dt: 21626.75ms | tok/sec: 24242.57\n",
      "step   190 | loss: 11.001354 | lr 1.6028e-04 | norm: 0.6336 | dt: 21967.58ms | tok/sec: 23866.44\n",
      "step   191 | loss: 10.995836 | lr 1.6112e-04 | norm: 0.6254 | dt: 21572.64ms | tok/sec: 24303.38\n",
      "step   192 | loss: 10.994827 | lr 1.6196e-04 | norm: 0.6350 | dt: 21636.11ms | tok/sec: 24232.08\n",
      "step   193 | loss: 10.991702 | lr 1.6280e-04 | norm: 0.6648 | dt: 21633.19ms | tok/sec: 24235.36\n",
      "step   194 | loss: 10.988829 | lr 1.6364e-04 | norm: 0.6643 | dt: 21635.68ms | tok/sec: 24232.56\n",
      "step   195 | loss: 10.992332 | lr 1.6448e-04 | norm: 0.6658 | dt: 21587.23ms | tok/sec: 24286.95\n",
      "step   196 | loss: 10.987989 | lr 1.6531e-04 | norm: 0.6666 | dt: 21605.16ms | tok/sec: 24266.79\n",
      "step   197 | loss: 10.992271 | lr 1.6615e-04 | norm: 0.6599 | dt: 21632.51ms | tok/sec: 24236.12\n",
      "step   198 | loss: 10.993647 | lr 1.6699e-04 | norm: 0.6483 | dt: 21630.82ms | tok/sec: 24238.01\n",
      "step   199 | loss: 10.995442 | lr 1.6783e-04 | norm: 0.6553 | dt: 21573.62ms | tok/sec: 24302.27\n",
      "step   200 | loss: 10.995227 | lr 1.6867e-04 | norm: 0.6705 | dt: 21581.53ms | tok/sec: 24293.36\n",
      "step   201 | loss: 10.996127 | lr 1.6951e-04 | norm: 0.6725 | dt: 21560.29ms | tok/sec: 24317.30\n",
      "step   202 | loss: 10.992614 | lr 1.7035e-04 | norm: 0.6908 | dt: 21580.53ms | tok/sec: 24294.49\n",
      "step   203 | loss: 10.993101 | lr 1.7119e-04 | norm: 0.6933 | dt: 21541.27ms | tok/sec: 24338.77\n",
      "step   204 | loss: 10.989392 | lr 1.7203e-04 | norm: 0.6957 | dt: 21625.94ms | tok/sec: 24243.47\n",
      "step   205 | loss: 10.995157 | lr 1.7287e-04 | norm: 0.7034 | dt: 21605.57ms | tok/sec: 24266.33\n",
      "step   206 | loss: 10.991325 | lr 1.7371e-04 | norm: 0.7233 | dt: 21631.94ms | tok/sec: 24236.75\n",
      "step   207 | loss: 10.994457 | lr 1.7455e-04 | norm: 0.7369 | dt: 21568.97ms | tok/sec: 24307.51\n",
      "step   208 | loss: 10.990418 | lr 1.7538e-04 | norm: 0.7628 | dt: 21649.23ms | tok/sec: 24217.39\n",
      "step   209 | loss: 10.992579 | lr 1.7622e-04 | norm: 0.7554 | dt: 21590.83ms | tok/sec: 24282.90\n",
      "step   210 | loss: 10.990874 | lr 1.7706e-04 | norm: 0.7450 | dt: 21661.70ms | tok/sec: 24203.45\n",
      "step   211 | loss: 10.990678 | lr 1.7790e-04 | norm: 0.7470 | dt: 21577.87ms | tok/sec: 24297.49\n",
      "step   212 | loss: 10.993976 | lr 1.7874e-04 | norm: 0.7582 | dt: 21630.07ms | tok/sec: 24238.85\n",
      "step   213 | loss: 10.996760 | lr 1.7958e-04 | norm: 0.7714 | dt: 21609.38ms | tok/sec: 24262.06\n",
      "step   214 | loss: 10.993965 | lr 1.8042e-04 | norm: 0.7812 | dt: 21684.05ms | tok/sec: 24178.51\n",
      "step   215 | loss: 10.992534 | lr 1.8126e-04 | norm: 0.7816 | dt: 21602.53ms | tok/sec: 24269.75\n",
      "step   216 | loss: 10.996877 | lr 1.8210e-04 | norm: 0.8018 | dt: 21630.22ms | tok/sec: 24238.68\n",
      "step   217 | loss: 10.994586 | lr 1.8294e-04 | norm: 0.8819 | dt: 21579.33ms | tok/sec: 24295.85\n",
      "step   218 | loss: 10.995973 | lr 1.8378e-04 | norm: 1.0042 | dt: 21632.27ms | tok/sec: 24236.38\n",
      "step   219 | loss: 10.995563 | lr 1.8462e-04 | norm: 1.0632 | dt: 21574.27ms | tok/sec: 24301.54\n",
      "step   220 | loss: 10.997716 | lr 1.8545e-04 | norm: 1.0085 | dt: 21617.56ms | tok/sec: 24252.87\n",
      "step   221 | loss: 10.995688 | lr 1.8629e-04 | norm: 1.0651 | dt: 21631.10ms | tok/sec: 24237.69\n",
      "step   222 | loss: 10.996796 | lr 1.8713e-04 | norm: 0.9802 | dt: 21708.67ms | tok/sec: 24151.09\n",
      "step   223 | loss: 10.997299 | lr 1.8797e-04 | norm: 0.9813 | dt: 21594.47ms | tok/sec: 24278.81\n",
      "step   224 | loss: 10.992102 | lr 1.8881e-04 | norm: 0.9286 | dt: 21634.15ms | tok/sec: 24234.28\n",
      "step   225 | loss: 10.998974 | lr 1.8965e-04 | norm: 0.9251 | dt: 21568.83ms | tok/sec: 24307.66\n",
      "step   226 | loss: 10.997359 | lr 1.9049e-04 | norm: 0.9249 | dt: 21630.84ms | tok/sec: 24237.99\n",
      "step   227 | loss: 10.996718 | lr 1.9133e-04 | norm: 0.9018 | dt: 21584.47ms | tok/sec: 24290.06\n",
      "step   228 | loss: 10.995052 | lr 1.9217e-04 | norm: 0.9193 | dt: 21643.21ms | tok/sec: 24224.13\n",
      "step   229 | loss: 10.993862 | lr 1.9301e-04 | norm: 0.9020 | dt: 21568.36ms | tok/sec: 24308.20\n",
      "step   230 | loss: 10.994772 | lr 1.9385e-04 | norm: 0.9030 | dt: 21668.55ms | tok/sec: 24195.81\n",
      "step   231 | loss: 10.994736 | lr 1.9469e-04 | norm: 0.9094 | dt: 21592.69ms | tok/sec: 24280.81\n",
      "step   232 | loss: 11.001987 | lr 1.9552e-04 | norm: 0.9009 | dt: 21645.81ms | tok/sec: 24221.22\n",
      "step   233 | loss: 10.999940 | lr 1.9636e-04 | norm: 0.9151 | dt: 21587.53ms | tok/sec: 24286.61\n",
      "step   234 | loss: 10.997597 | lr 1.9720e-04 | norm: 0.9339 | dt: 21638.41ms | tok/sec: 24229.50\n",
      "step   235 | loss: 11.000561 | lr 1.9804e-04 | norm: 0.9267 | dt: 21600.76ms | tok/sec: 24271.74\n",
      "step   236 | loss: 10.997190 | lr 1.9888e-04 | norm: 0.9406 | dt: 21621.08ms | tok/sec: 24248.93\n",
      "step   237 | loss: 11.000930 | lr 1.9972e-04 | norm: 0.9400 | dt: 21602.56ms | tok/sec: 24269.71\n",
      "step   238 | loss: 11.002624 | lr 2.0056e-04 | norm: 0.9350 | dt: 21622.49ms | tok/sec: 24247.35\n",
      "step   239 | loss: 10.996210 | lr 2.0140e-04 | norm: 0.9417 | dt: 21151.95ms | tok/sec: 24786.74\n",
      "step   240 | loss: 10.997239 | lr 2.0224e-04 | norm: 0.9466 | dt: 21179.74ms | tok/sec: 24754.22\n",
      "step   241 | loss: 11.002266 | lr 2.0308e-04 | norm: 0.9595 | dt: 21141.36ms | tok/sec: 24799.16\n",
      "step   242 | loss: 11.004505 | lr 2.0392e-04 | norm: 0.9837 | dt: 21190.27ms | tok/sec: 24741.92\n",
      "step   243 | loss: 11.001319 | lr 2.0476e-04 | norm: 1.0016 | dt: 21187.90ms | tok/sec: 24744.69\n",
      "step   244 | loss: 11.002215 | lr 2.0559e-04 | norm: 1.0417 | dt: 21195.80ms | tok/sec: 24735.46\n",
      "step   245 | loss: 10.999645 | lr 2.0643e-04 | norm: 1.0798 | dt: 21188.98ms | tok/sec: 24743.43\n",
      "step   246 | loss: 10.997108 | lr 2.0727e-04 | norm: 1.0900 | dt: 21192.21ms | tok/sec: 24739.66\n",
      "step   247 | loss: 11.001155 | lr 2.0811e-04 | norm: 1.1389 | dt: 21242.95ms | tok/sec: 24680.57\n",
      "step   248 | loss: 11.000037 | lr 2.0895e-04 | norm: 1.1587 | dt: 21282.06ms | tok/sec: 24635.21\n",
      "step   249 | loss: 10.998837 | lr 2.0979e-04 | norm: 1.2306 | dt: 21267.37ms | tok/sec: 24652.23\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "x:torch.Size([4, 1024]), y:torch.Size([4, 1024])\n",
      "validation loss: 11.0073\n",
      "Downloading https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl to /home/ec2-user/SageMaker/hellaswag/hellaswag_val.jsonl...Downloading https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl to /home/ec2-user/SageMaker/hellaswag/hellaswag_val.jsonl...Downloading https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl to /home/ec2-user/SageMaker/hellaswag/hellaswag_val.jsonl...Downloading https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl to /home/ec2-user/SageMaker/hellaswag/hellaswag_val.jsonl...Downloading https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl to /home/ec2-user/SageMaker/hellaswag/hellaswag_val.jsonl...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl to /home/ec2-user/SageMaker/hellaswag/hellaswag_val.jsonl...\n",
      "Downloading https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl to /home/ec2-user/SageMaker/hellaswag/hellaswag_val.jsonl...\n",
      "Downloading https://raw.githubusercontent.com/rowanz/hellaswag/master/data/hellaswag_val.jsonl to /home/ec2-user/SageMaker/hellaswag/hellaswag_val.jsonl...\n",
      "/home/ec2-user/SageMaker/hellaswag/hellaswag_val.jsonl: 11.7MiB [00:00, 64.7MiB/s]\n",
      "\n",
      "\n",
      "/home/ec2-user/SageMaker/hellaswag/hellaswag_val.jsonl: 11.7MiB [00:00, 64.3MiB/s]\n",
      "/home/ec2-user/SageMaker/hellaswag/hellaswag_val.jsonl: 11.7MiB [00:00, 64.1MiB/s]\n",
      "/home/ec2-user/SageMaker/hellaswag/hellaswag_val.jsonl: 11.7MiB [00:00, 64.0MiB/s]\n",
      "/home/ec2-user/SageMaker/hellaswag/hellaswag_val.jsonl: 11.7MiB [00:00, 63.2MiB/s]\n",
      "/home/ec2-user/SageMaker/hellaswag/hellaswag_val.jsonl: 11.0MiB [00:00, 56.5MiB/s]Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/train_ideal.py\", line 459, in <module>\n",
      "  File \"/home/ec2-user/SageMaker/train_ideal.py\", line 459, in <module>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/train_ideal.py\", line 459, in <module>\n",
      "  File \"/home/ec2-user/SageMaker/train_ideal.py\", line 459, in <module>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/train_ideal.py\", line 459, in <module>\n",
      "    with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 306, in __init__\n",
      "    with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
      "      File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 306, in __init__\n",
      "with torch.autocast(device_type=device_type, dtype=torch.bfloat16):    \n",
      "with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 306, in __init__\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 306, in __init__\n",
      "    with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 306, in __init__\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/train_ideal.py\", line 459, in <module>\n",
      "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):        raise RuntimeError(\n",
      "raise RuntimeError(    raise RuntimeError(\n",
      "\n",
      "raise RuntimeError(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 306, in __init__\n",
      "\n",
      "    RuntimeErrorRuntimeErrorRuntimeErrorRuntimeErrorraise RuntimeError(: : : : \n",
      "Current CUDA Device does not support bfloat16. Please switch dtype to float16.Current CUDA Device does not support bfloat16. Please switch dtype to float16.Current CUDA Device does not support bfloat16. Please switch dtype to float16.Current CUDA Device does not support bfloat16. Please switch dtype to float16.\n",
      "\n",
      "RuntimeError\n",
      "\n",
      ": Current CUDA Device does not support bfloat16. Please switch dtype to float16.\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Current CUDA Device does not support bfloat16. Please switch dtype to float16.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/train_ideal.py\", line 459, in <module>\n",
      "    with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 306, in __init__\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Current CUDA Device does not support bfloat16. Please switch dtype to float16.\n",
      "/home/ec2-user/SageMaker/hellaswag/hellaswag_val.jsonl: 11.7MiB [00:00, 56.6MiB/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/SageMaker/train_ideal.py\", line 459, in <module>\n",
      "    with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/amp/autocast_mode.py\", line 306, in __init__\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Current CUDA Device does not support bfloat16. Please switch dtype to float16.\n",
      "[2024-08-03 07:01:07,554] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 20666 closing signal SIGTERM\n",
      "[2024-08-03 07:01:07,718] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 20662) of binary: /home/ec2-user/anaconda3/envs/pytorch_p310/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/bin/torchrun\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('torch==2.2.0', 'console_scripts', 'torchrun')())\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/distributed/run.py\", line 812, in main\n",
      "    run(args)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/distributed/run.py\", line 803, in run\n",
      "    elastic_launch(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "train_ideal.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2024-08-03_07:01:07\n",
      "  host      : ip-172-16-38-83.ec2.internal\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 20663)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[2]:\n",
      "  time      : 2024-08-03_07:01:07\n",
      "  host      : ip-172-16-38-83.ec2.internal\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : 1 (pid: 20664)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[3]:\n",
      "  time      : 2024-08-03_07:01:07\n",
      "  host      : ip-172-16-38-83.ec2.internal\n",
      "  rank      : 3 (local_rank: 3)\n",
      "  exitcode  : 1 (pid: 20665)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[4]:\n",
      "  time      : 2024-08-03_07:01:07\n",
      "  host      : ip-172-16-38-83.ec2.internal\n",
      "  rank      : 5 (local_rank: 5)\n",
      "  exitcode  : 1 (pid: 20667)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[5]:\n",
      "  time      : 2024-08-03_07:01:07\n",
      "  host      : ip-172-16-38-83.ec2.internal\n",
      "  rank      : 6 (local_rank: 6)\n",
      "  exitcode  : 1 (pid: 20668)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[6]:\n",
      "  time      : 2024-08-03_07:01:07\n",
      "  host      : ip-172-16-38-83.ec2.internal\n",
      "  rank      : 7 (local_rank: 7)\n",
      "  exitcode  : 1 (pid: 20669)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-08-03_07:01:07\n",
      "  host      : ip-172-16-38-83.ec2.internal\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 20662)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!torchrun --standalone --nproc_per_node=8 train_ideal.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1618df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93b6a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --standalone --nproc_per_node=1 train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d1ef1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TORCH_USE_CUDA_DSA=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cbdfe90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem      Size  Used Avail Use% Mounted on\r\n",
      "devtmpfs        241G     0  241G   0% /dev\r\n",
      "tmpfs           241G  8.0K  241G   1% /dev/shm\r\n",
      "tmpfs           241G  936K  241G   1% /run\r\n",
      "tmpfs           241G     0  241G   0% /sys/fs/cgroup\r\n",
      "/dev/xvda1      135G   79G   57G  59% /\r\n",
      "tmpfs            49G     0   49G   0% /run/user/0\r\n",
      "/dev/xvdf        20G  6.0G   13G  32% /home/ec2-user/SageMaker\r\n",
      "tmpfs            49G     0   49G   0% /run/user/1002\r\n",
      "tmpfs            49G     0   49G   0% /run/user/1001\r\n",
      "tmpfs            49G     0   49G   0% /run/user/1000\r\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5efdd703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.43.3-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, tokenizers, transformers\n",
      "Successfully installed safetensors-0.4.3 tokenizers-0.19.1 transformers-4.43.3\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fa8c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
